------------------------learning_rate = 0.05------------------------------------------------------------------------
Using:  sigmoid
After 1000 iterations, the total error is 20.61650712727831
The final weight vectors are (starting from input to output layers)
[[-0.39213539  0.40142812 -1.82612523  0.07105106]
 [-0.08397051 -0.7679167  -1.29838746 -0.43213609]
 [-3.38958422  0.62689279 -1.37394823  5.15581582]]
[[-0.17219847  4.23635041]
 [-0.1382813  -0.29531138]
 [ 0.13477601  2.63364283]
 [-0.29299098 -4.97802696]]
[[ 1.70888117]
 [-4.90965786]]
--Prediction--
total error in test dataset 5.371589543146545
Using:  tanh
After 1000 iterations, the total error is 89.49962715743175
The final weight vectors are (starting from input to output layers)
[[-0.44381523 -0.26299875 -0.75627919 -0.89365716]
 [-1.06183441 -1.51339642 -0.38141317 -0.73994751]
 [-0.62838683 -0.61593305 -0.64731512 -0.64777493]]
[[-1.03001665  1.4554593 ]
 [-1.09280899  1.61685312]
 [-1.59292711  0.77448431]
 [-1.90532032  1.07808361]]
[[ 6.81274169]
 [-4.05893894]]
--Prediction--
total error in test dataset 22.999989675872058
Using:  ReLu
After 1000 iterations, the total error is 32.5
The final weight vectors are (starting from input to output layers)
[[ -0.16595599 -13.72564709  -0.99977125  -0.39933688]
 [ -0.70648822  -9.54992932  -0.62747958  -0.31841335]
 [ -0.20646505  -2.07926994  -0.16161097   0.37402007]]
[[ -0.5910955    0.75623487]
 [ -0.94522481 -14.27607571]
 [ -0.1653904    0.11737966]
 [ -0.71922612  -0.66222943]]
[[ 0.60148914]
 [-5.52206283]]
--Prediction--
total error in test dataset 8.0

------------------------learning_rate = 0.1------------------------------------------------------------------------
Using:  sigmoid
After 1000 iterations, the total error is 19.96341769018457
The final weight vectors are (starting from input to output layers)
[[-0.11021391  0.81237304 -4.14631949 -0.68071822]
 [ 0.02340483 -2.55160002 -2.09447156  0.4081407 ]
 [-3.63163615  0.92791565  0.35544684  6.18924377]]
[[ 0.85448584  4.49262333]
 [ 1.31336456 -1.4432851 ]
 [-1.25204381  6.05132916]
 [-1.86211989 -4.94044027]]
[[ 2.6268455 ]
 [-6.29115277]]
--Prediction--
total error in test dataset 5.349446523189987
Using:  tanh
After 1000 iterations, the total error is 89.4999999979525
The final weight vectors are (starting from input to output layers)
[[-5.21453284 -4.80926831 -2.84301899 -1.74292305]
 [-6.66926786 -6.39392811 -2.90427042 -1.53667007]
 [-1.22374567 -1.21245886 -0.48843152 -0.29015787]]
[[  3.20649163  -0.9416967 ]
 [ -1.9498597    0.01545384]
 [ -7.16916881  -1.32286   ]
 [-11.60139697  -0.1079815 ]]
[[4.88647375]
 [8.80448808]]
--Prediction--
total error in test dataset 22.999999999803574
Using:  ReLu
After 1000 iterations, the total error is 32.5
The final weight vectors are (starting from input to output layers)
[[ -0.16595599 -23.85462727  -0.99977125  -0.39567496]
 [ -0.70648822  -5.56766974  -0.62747958  -0.32687217]
 [ -0.20646505  -2.16942238  -0.16161097   0.42705861]]
[[ -0.5910955    0.75623487]
 [ -0.94522481 -20.58505653]
 [ -0.1653904    0.11737966]
 [ -0.71922612  -0.72062244]]
[[ 0.60148914]
 [-5.82637852]]
--Prediction--
total error in test dataset 8.0

------------------------learning_rate = 0.01------------------------------------------------------------------------
Using:  sigmoid
After 1000 iterations, the total error is 23.621475954602502
The final weight vectors are (starting from input to output layers)
[[-0.44548297  0.42628812 -1.11414758 -0.05912653]
 [-0.65982575 -0.79992438 -0.62197637 -0.28804217]
 [-0.76889024  0.03926363 -0.39304965  1.01355705]]
[[-0.52638414  1.13151551]
 [-1.022838    0.09560112]
 [-0.07379871  0.59474691]
 [-0.84451135 -1.05222251]]
[[-0.44664044]
 [-1.68414201]]
--Prediction--
total error in test dataset 5.893291968742983
Using:  tanh
After 1000 iterations, the total error is 20.740435553660447
The final weight vectors are (starting from input to output layers)
[[ 0.25014653  0.74368833 -0.81942888  0.03004136]
 [-1.3806378  -0.56077693 -0.62822098 -0.12268889]
 [ 0.05783524 -0.34436949 -0.26189249 -2.45447351]]
[[ 0.17435306  1.3593461 ]
 [-0.56330543  0.85967077]
 [-0.66301389  0.00741997]
 [-1.44715903 -0.46189156]]
[[0.78140753]
 [0.32600349]]
--Prediction--
total error in test dataset 5.433437456413059
Using:  ReLu
After 1000 iterations, the total error is 32.5
The final weight vectors are (starting from input to output layers)
[[-0.16595599 -2.14290639 -0.99977125 -0.39603614]
 [-0.70648822 -2.24782096 -0.62747958 -0.31964777]
 [-0.20646505  0.04056334 -0.16161097  0.44156751]]
[[-0.5910955   0.75623487]
 [-0.94522481 -0.80730394]
 [-0.1653904   0.11737966]
 [-0.71922612 -0.78335766]]
[[0.60148914]
 [0.88219471]]
--Prediction--
total error in test dataset 8.0

------------------------learning_rate = 0.001------------------------------------------------------------------------
Using:  sigmoid
After 1000 iterations, the total error is 23.80568680383707
The final weight vectors are (starting from input to output layers)
[[-0.15404326  0.4522832  -0.99749485 -0.39389419]
 [-0.68814431 -0.801245   -0.62416316 -0.31154629]
 [-0.23656683  0.06975625 -0.16738556  0.40368325]]
[[-0.60738713  0.87523539]
 [-0.96920258  0.45059865]
 [-0.17656311  0.22800395]
 [-0.74401963 -0.50796052]]
[[-0.47372249]
 [-1.46115606]]
--Prediction--
total error in test dataset 5.934257647411452
Using:  tanh
After 1000 iterations, the total error is 21.405708874176327
The final weight vectors are (starting from input to output layers)
[[-0.32978214  0.21628411 -1.0187676  -0.21355605]
 [-0.59144169 -0.67579996 -0.5799131  -0.24693444]
 [ 0.34886567 -0.02949105 -0.28467041 -0.94471081]]
[[-0.42754699  0.88315735]
 [-0.70325611  0.2863096 ]
 [-0.27829593 -0.05678424]
 [-0.85887946 -0.83891571]]
[[0.74355312]
 [1.13475926]]
--Prediction--
total error in test dataset 5.073448249061367
Using:  ReLu
After 1000 iterations, the total error is 32.5
The final weight vectors are (starting from input to output layers)
[[-0.16595599 -0.25289264 -0.99977125 -0.42662441]
 [-0.70648822 -0.33602041 -0.62747958 -0.33968469]
 [-0.20646505  0.0927146  -0.16161097  0.30060799]]
[[-0.5910955   0.75623487]
 [-0.94522481  2.72665133]
 [-0.1653904   0.11737966]
 [-0.71922612 -0.40351607]]
[[0.60148914]
 [2.38455092]]
--Prediction--
total error in test dataset 7.955700419828431

------------------------learning_rate = 0.0001------------------------------------------------------------------------
Using:  sigmoid
After 1000 iterations, the total error is 30.691629479026773
The final weight vectors are (starting from input to output layers)
[[-0.17210845  0.44718259 -0.99917694 -0.37323841]
 [-0.71302547 -0.80778182 -0.62675722 -0.28397156]
 [-0.20711469  0.07806105 -0.16158599  0.37231416]]
[[-0.64202587  0.67560737]
 [-1.00269183  0.25002238]
 [-0.20786032  0.05005185]
 [-0.77393334 -0.69013119]]
[[ 0.14023873]
 [-0.04137767]]
--Prediction--
total error in test dataset 7.795947206684683
Using:  tanh
After 1000 iterations, the total error is 23.240400556383204
The final weight vectors are (starting from input to output layers)
[[-0.18976364  0.35300762 -1.01571079 -0.53103508]
 [-0.72414554 -0.82825376 -0.62240786 -0.32891511]
 [-0.0067565   0.05428024 -0.20120938 -0.11829257]]
[[-0.63142743  0.71409491]
 [-0.88120951  0.41652587]
 [-0.30156009 -0.01669862]
 [-0.73699079 -0.55319915]]
[[0.610888  ]
 [0.88000341]]
--Prediction--
total error in test dataset 5.870173466040647
Using:  ReLu
After 1000 iterations, the total error is 23.728481064197254
The final weight vectors are (starting from input to output layers)
[[-0.16595599  0.54819767 -0.99977125 -0.41280157]
 [-0.70648822 -0.4100069  -0.62747958 -0.32291857]
 [-0.20646505  0.50527537 -0.16161097  0.34296948]]
[[-0.5910955   0.75623487]
 [-0.94522481  1.01881039]
 [-0.1653904   0.11737966]
 [-0.71922612 -0.55719917]]
[[0.60148914]
 [1.49453311]]
--Prediction--
total error in test dataset 6.933643829038909

------------------------learning_rate = 0.00001------------------------------------------------------------------------
Using:  sigmoid
After 1000 iterations, the total error is 41.24582986363339
The final weight vectors are (starting from input to output layers)
[[-0.16795199  0.44139786 -0.99986271 -0.3907513 ]
 [-0.70854366 -0.81448506 -0.62756764 -0.30396668]
 [-0.20667685  0.077711   -0.16162145  0.37093969]]
[[-0.60058097  0.73752032]
 [-0.95595755  0.31978241]
 [-0.17324346  0.10185371]
 [-0.72934454 -0.62370327]]
[[0.54571598]
 [0.817579  ]]
--Prediction--
total error in test dataset 10.55742649261875
Using:  tanh
After 1000 iterations, the total error is 25.677619951370147
The final weight vectors are (starting from input to output layers)
[[-0.14928051  0.42110857 -1.00215654 -0.46369725]
 [-0.69546832 -0.81989191 -0.62750408 -0.34341614]
 [-0.18678323  0.06647986 -0.16342884  0.31096963]]
[[-0.614481    0.7146287 ]
 [-0.93748177  0.34461733]
 [-0.21677802  0.03642247]
 [-0.73164845 -0.62005687]]
[[0.65644596]
 [0.90529829]]
--Prediction--
total error in test dataset 6.629778232443004
Using:  ReLu
After 1000 iterations, the total error is 29.94774140339635
The final weight vectors are (starting from input to output layers)
[[-0.16595599  0.49239321 -0.99977125 -0.39533485]
 [-0.70648822 -0.80261041 -0.62747958 -0.30887855]
 [-0.20646505  0.09526703 -0.16161097  0.370439  ]]
[[-0.5910955   0.75623487]
 [-0.94522481  0.56129082]
 [-0.1653904   0.11737966]
 [-0.71922612 -0.60379702]]
[[0.60148914]
 [1.14796864]]
--Prediction--
total error in test dataset 7.952912516381637